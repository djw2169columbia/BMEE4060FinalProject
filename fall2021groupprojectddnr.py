# -*- coding: utf-8 -*-
"""Fall2021GroupProjectDDNR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m9vD70GR32KYfH3ac1QjYvHHSfZK5UT3
"""

#@title Install prerequsite: you may have to do Runtime -> Restart runtime after the installation

!pip install --upgrade pip
!pip install --upgrade setuptools wheel
!pip install --upgrade "mxnet<2.0.0"
!pip install autogluon

import autogluon

#@title Mount Google Drive (You don't need to run this if you are running notebooks on your laptop)

from google.colab import drive

# The following command will prompt a URL for you to click and obtain the
# authorization code

drive.mount("/content/drive")

# Set up data folder
from pathlib import Path

# Change this to where you put your hw2 files
DATA = Path("/content/drive/My Drive/project")

"""# Project part 1

## Reclassifying Mutations

First the existing mutation files from the paper were loaded. The mutations from the files were combined a kept seperate by gene. The total mutations for each type of gene found in this analysis match the mutations statistics record in the meta files from the paper.
"""

import pandas as pd
snvsfile = DATA / "snvs.txt"
snvs = pd.read_csv(snvsfile, sep="\t", comment="#")
snvs = snvs[['Gene', 'Sample_ID']]
snvs = snvs.dropna()

import pandas as pd
indelsfile = DATA / "indels.txt"
indels = pd.read_csv(indelsfile, sep="\t", comment="#")
indels = indels[['Gene', 'Sample_ID']]
indels = indels.dropna()

groupedsnvs = snvs.groupby(by=['Gene']).count()
groupedsnvs = groupedsnvs.sort_values(by = ["Sample_ID"], ascending=False)
groupedsnvs.head()

groupedindels = indels.groupby(by=['Gene']).count()
groupedindels = groupedindels.sort_values(by = ["Sample_ID"], ascending=False)
groupedindels.head()

concat = pd.concat([snvs, indels])

count = concat.groupby(by='Gene')
count = count.describe()
count = count['Sample_ID']
count = count.sort_values(by ='unique', ascending=False)
count = count[['unique']]
count.head()

count['snvs'] = groupedsnvs.Sample_ID
count['indel'] = groupedindels.Sample_ID
summary = count.fillna(0)
summary = summary.astype(int)
summary

"""# Project Part 2

## Machine Learning

The paper uses methylation data to divide the atrt tumors into three seperate subcatagories. The gene expression data contains similar data but unlike the methylation data it is not specific to a location. We started by loading the gene expression and methylation data for 49 cases from the 180 atrt patients in the paper. The subtype labels from the paper are then matched with these 49 samples by sample name. The 49 samples are now split into 3 groups and can be used in a multiclass training method by autogluon. The data was split into 40 training cases and 9 test cases, 3 from each class.
"""

import pandas as pd

gefile = DATA / "GSE70678_matrix.txt"
methfile = DATA / "GSE70460_series_matrix.txt.gz"

ge = pd.read_csv(gefile, sep="\t", na_values="unknown", index_col=0)
meth = pd.read_csv(methfile, sep="\t", na_values="unknown", index_col=0, header= 64)
gemeta = ge.head(36)
ge.columns = ge.iloc[35]
ge = ge.iloc[36:-1 ,:]

gemeta.head()

ge.head()

meth.head()

"""Thte meta file is loaded in here along with other variable to include in the autogluon training in case the help. """

classfile = DATA / "metaandclassify.txt"
classified = pd.read_csv(classfile, sep="\t", comment="#",index_col=0)
classified = classified[['Molecular Subgroup (Consensus)','Gender (F= FeMale, M=Male)','Age at diagnosis']]
classified.head()

meta = gemeta.transpose()
classified = classified[classified.index.isin(meta.index)]
classified.sort_index(inplace=True)
meta.sort_index(inplace=True)
meta['Class'] = classified['Molecular Subgroup (Consensus)']
meta['Age'] = classified['Age at diagnosis']
meta['Gender'] = classified['Gender (F= FeMale, M=Male)']
meta = meta.rename(columns={"!Sample_geo_accession": "SampleGeo"})
meta.rename(columns = {list(meta)[9]: 'cha1'}, inplace = True)

meta.head()

"""### Split test set

3 samples are chosen from each class of tumor to become the test dataset
"""

# pick 3 samples from each class (MYC, SHH, TYR) as test set for evaluation in the end
test_sample_id = meta.groupby("Class").apply(lambda g: g.SampleGeo.iloc[-3:])
test_sample_id

# define train and test set
meta["subset"] = "train"
meta.loc[meta.SampleGeo.isin(test_sample_id), "subset"] = "test"

meta.groupby(["subset", "Class"]).SampleGeo.nunique()

"""### Feature selection

Using the method from class we calculate the mutual information between each gene and the severity using scikit-learn's [`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) function. All genes are ranked and a cutoff is selected.
"""

#=======================================================
# Your code here
# Perform feature selection using mutual_info_classif 
#=======================================================
di = {"TYR": 0, "SHH": 1,"MYC":2}
meta = meta.replace({"Class": di})
target = (meta["Class"]).astype(int)

# Creating trianing and testing data. 


y_train = target[(meta.subset == "train")].values
y_test = target[(meta.subset == "test")].values
x_train = ge[meta[meta.subset == "train"].SampleGeo].transpose()
x_test = ge[meta[meta.subset == "test"].SampleGeo].transpose()

from sklearn.feature_selection import mutual_info_classif

midf = pd.DataFrame({
    "feature": x_train.columns,
    "mi": mutual_info_classif(x_train, y_train,)
}).sort_values("mi", ascending=False)

import matplotlib.pyplot as plt
import numpy as np
plt.plot(np.arange(100), midf.mi.iloc[:100])

selected_genes = midf.feature.loc[midf.mi > 0.62]
len(selected_genes)

"""### Training an ATRT classifier using AutoML

Procedures used in class were repeated here to create an AutoML model for classifying ATRT data.
"""

#===========================================================================
# Your code here
# Train a classification model using AutoGluon TabularPrediction module with
# features selected by the RandomForestClassifier
#===========================================================================

from autogluon.tabular import TabularPredictor

train_data = x_train.loc[:, selected_genes].copy()
train_data["Class"] = y_train

predictor = TabularPredictor(label="Class", path="good_quality_atrt").fit( 
    train_data=train_data,
    presets="good_quality_faster_inference_only_refit",
)

"""Performance scores are calculated to evaluate the model. """

from sklearn.metrics import (accuracy_score, balanced_accuracy_score, 
                             roc_auc_score, f1_score)
def performance_scores(y_true, y_pred_score, y_pred=None):
    # We can find which class has the highest score as its predicted class
    if y_pred is None:
        y_pred = y_pred_score.idxmax(axis=1)
        
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "balanced_accuracy": balanced_accuracy_score(y_true, y_pred),
        #"auroc": roc_auc_score(y_true, y_pred_score[:, 1], average="weighted",
        #                       multi_class="ovr"),
        "f1": f1_score(y_true, y_pred, average="weighted")
    }

test_data = x_test.loc[:, selected_genes].copy()
y_pred_score = predictor.predict_proba(test_data).values
y_pred = predictor.predict(test_data)
y_pred
performance_scores(y_test, y_pred_score, y_pred)

y_pred_score

"""Now also plot the confusion matrix to show correct and incorrect predictions in the test set"""

#============================================================================
# Your code here
# Plot confusion matrix for the trained model
#============================================================================

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(cm, display_labels=["TYR", "SHH","MYC"])
disp.plot(cmap="Blues")

"""The models are displayed here in a leader board to show which performed best.

"""

predictor.leaderboard()

featimp = predictor.feature_importance(train_data, num_shuffle_sets=10)
featimp[featimp.importance > 0].sort_values("p_value")

"""The best model in this dataset was the 'ExtraTreesEntr_BAG_L1_FULL' and we can see that the most important features were all the at genes, which is expected in atrt patients.

Additional features from the meta file can be added to the analysis, but it can be seen that they are not selected and make no difference on the AutoML model.
"""

feature_columns = [
    'Age', 'Gender'
]

# Creating training and testing data. 

x_train = pd.concat([
    meta.loc[meta.subset == "train", feature_columns + ["SampleGeo"]].set_index("SampleGeo"),
    ge[meta[meta.subset == "train"].SampleGeo].transpose()
], axis=1)
y_train = target[(meta.subset == "train")].values
x_test = pd.concat([
    meta.loc[meta.subset == "test", feature_columns + ["SampleGeo"]].set_index("SampleGeo"),
    ge[meta[meta.subset == "test"].SampleGeo].transpose()
], axis=1)
y_test = target[(meta.subset == "test")].values

# for categorical variable, we simply encode it as 0 for male, 1 for female
x_train["Gender"] = (x_train.Gender == "F").astype(int)
x_test["Gender"] = (x_test.Gender == "F").astype(int)

# for missing data, we'll use the mean imputation
from sklearn.impute import SimpleImputer

imputer = SimpleImputer()
x_train_mi = imputer.fit_transform(x_train)
x_test_mi = imputer.transform(x_test)

midf = pd.DataFrame({
    "feature": x_train.columns,
    "mi": mutual_info_classif(x_train_mi, y_train)
}).sort_values("mi", ascending=False)

plt.plot(np.arange(100), midf.mi.iloc[:100])

selected_features = midf.loc[midf.mi > 0.62, "feature"]

# there are no features in the metadata selected based on MI
[x for x in meta.columns if x in selected_features]

from autogluon.tabular import TabularPredictor

train_data = x_train[selected_features].copy()
train_data["Class"] = y_train

predictor = TabularPredictor(
    label="Class", path="good_quality_atrt_meta"
).fit( 
    train_data=train_data,
    presets="good_quality_faster_inference_only_refit",
)

test_data = x_test[selected_features].copy()
y_pred_score = predictor.predict_proba(test_data).values
y_pred = predictor.predict(test_data)

performance_scores(y_test, y_pred_score, y_pred)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(cm, display_labels=["TYR", "SHH","MYC"])
disp.plot(cmap="Blues")

"""The features in the metadata did not have high enough MI with the severity and thus none of the features were selected into model training. The performance of the model remains the same given the same feature selection criteria."""